---
alwaysApply: true
---

# Test Rules and Guidelines

## Overview

This document defines comprehensive rules for creating testable code and managing tests in Python projects. These rules ensure that code is maintainable, reliable, and follows best practices for testing.

## Code Structure for Testability

### 1. Dependency Injection
**ALWAYS inject dependencies rather than creating them internally.**

```python
# Good - Testable
class QualityAssuranceRunner:
    def __init__(self, executor: LayeredCommand, logger: Logger):
        self.executor = executor
        self.logger = logger

# Bad - Not testable
class QualityAssuranceRunner:
    def __init__(self):
        self.executor = LayeredCommand()  # Hard to mock
        self.logger = logging.getLogger(__name__)  # Hard to control
```

### 2. Interface Segregation
**Create small, focused interfaces that are easy to mock.**

```python
# Good - Small, focused interface
class Executor(Protocol):
    def execute(self, command: str) -> Tuple[int, str, str]:
        ...

# Bad - Large, complex interface
class Executor(Protocol):
    def execute(self, command: str) -> Tuple[int, str, str]:
        ...
    def install_dependencies(self, deps: List[str]) -> None:
        ...
    def setup_environment(self, env: Dict[str, str]) -> None:
        ...
```

### 3. Pure Functions
**Prefer pure functions that are deterministic and have no side effects.**

```python
# Good - Pure function
def calculate_score(passed: int, total: int) -> float:
    if total == 0:
        return 0.0
    return passed / total

# Bad - Function with side effects
def calculate_score(passed: int, total: int) -> float:
    if total == 0:
        print("No tests found")  # Side effect
        return 0.0
    return passed / total
```

### 4. Error Handling
**Make error conditions explicit and testable.**

```python
# Good - Explicit error handling
def parse_version(version_str: str) -> Version:
    try:
        parts = version_str.split(".")
        if len(parts) != 3:
            raise ValueError(f"Version must have 3 parts, got {len(parts)}")
        return Version(*map(int, parts))
    except ValueError as e:
        raise ValueError(f"Invalid version format: {version_str}") from e

# Bad - Implicit error handling
def parse_version(version_str: str) -> Version:
    parts = version_str.split(".")
    return Version(*map(int, parts))  # Can fail silently
```

### 5. Logging in Testable Code
**Use logging strategically - it's valuable for debugging and understanding code flow.**

```python
# Good - Strategic logging
def calculate_score(passed: int, total: int) -> float:
    logger.debug("Calculating score: %d passed out of %d total", passed, total)
    if total == 0:
        logger.warning("No tests found, returning default score")
        return 0.0
    
    score = passed / total
    logger.info("Test score calculated: %.2f", score)
    return score

# Good - Logging with lazy evaluation for performance
def process_large_dataset(items: List[str]) -> List[str]:
    logger.info("Processing %d items", len(items))  # Lazy evaluation
    for i, item in enumerate(items):
        if i % 1000 == 0:  # Log progress periodically
            logger.debug("Processed %d/%d items", i, len(items))
        # Process item
    logger.info("Processing completed")
```

**When to use logging in tests:**
- **Debug test failures**: Logs help understand why a test failed
- **Verify behavior**: Logs confirm the code is working as expected
- **Document flow**: Logs show the execution path through complex code
- **Performance monitoring**: Log timing information for performance tests

## Test Organization

### 1. Test Structure
**Follow the AAA pattern: Arrange, Act, Assert.**

```python
def test_calculate_score_perfect_score(self):
    # Arrange
    passed = 10
    total = 10
    
    # Act
    result = calculate_score(passed, total)
    
    # Assert
    self.assertEqual(1.0, result)  # expected, actual
```

### 2. Test Naming
**Use descriptive test names that explain the scenario and expected outcome.**

```python
# Good - Descriptive names
def test_calculate_score_with_zero_total_returns_zero(self):
def test_parse_version_with_invalid_format_raises_value_error(self):
def test_quality_assurance_runner_with_failing_tests_raises_exit_early_error(self):

# Bad - Vague names
def test_calculate_score(self):
def test_parse_version(self):
def test_runner(self):
```

### 3. Test Categories
**Organize tests by category and complexity.**

- **Unit Tests**: Test individual functions/classes in isolation
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete workflows
- **Property Tests**: Test invariants and properties
- **Performance Tests**: Test timing and resource usage

### 4. Test Data Management
**Use factories and builders for test data creation.**

```python
# Good - Test data factory
class VersionFactory:
    @staticmethod
    def create(major: int = 1, minor: int = 0, patch: int = 0) -> Version:
        return Version(major, minor, patch)
    
    @staticmethod
    def create_invalid() -> str:
        return "invalid.version"

# Usage in tests
def test_version_creation(self):
    version = VersionFactory.create(major=2, minor=1, patch=3)
    self.assertEqual(2, version.major)  # expected, actual
```

## Testing Strategies

### 1. Mocking Guidelines
**Mock external dependencies, not the code under test.**

```python
# Good - Mock external dependencies
@patch('quickpub.proxy.os_system')
def test_runner_execution(self, mock_os_system):
    mock_os_system.return_value = (0, "output", "error")
    
    runner = QualityAssuranceRunner()
    result = runner.run("target")
    
    self.assertEqual(1.0, result)  # expected, actual

# Bad - Mock the code under test
@patch('quickpub.quality_assurance_runner.QualityAssuranceRunner._calculate_score')
def test_runner_execution(self, mock_calculate):
    mock_calculate.return_value = 1.0
    # This doesn't test the actual logic
```

### 2. Async Testing
**Use proper async test patterns for async code.**

```python
# Good - Async test pattern
class TestAsyncRunner(AsyncAutoCWDTestCase):
    async def asyncSetUp(self):
        self.runner = AsyncQualityAssuranceRunner()
    
    async def test_async_execution(self):
        result = await self.runner.run_async("target")
        self.assertEqual(1.0, result)  # expected, actual
```

### 3. Error Testing
**Test both success and failure scenarios.**

```python
def test_success_scenario(self):
    # Test normal operation
    result = function_under_test(valid_input)
    self.assertEqual(expected_output, result)  # expected, actual

def test_error_scenario(self):
    # Test error conditions
    with self.assertRaises(ValueError):
        function_under_test(invalid_input)
```

### 4. Boundary Testing
**Test edge cases and boundary conditions.**

```python
def test_boundary_conditions(self):
    # Test minimum values
    result = calculate_score(0, 0)
    self.assertEqual(0.0, result)  # expected, actual
    
    # Test maximum values
    result = calculate_score(100, 100)
    self.assertEqual(1.0, result)  # expected, actual
    
    # Test edge cases
    result = calculate_score(1, 1)
    self.assertEqual(1.0, result)  # expected, actual
```

### 5. Logging in Tests
**Use logging strategically in tests for debugging and verification.**

```python
# Good - Logging for test debugging
def test_complex_workflow(self):
    logger.info("Starting complex workflow test")
    
    # Test setup
    logger.debug("Setting up test data")
    test_data = create_test_data()
    
    # Execute workflow
    logger.debug("Executing workflow with %d items", len(test_data))
    result = execute_workflow(test_data)
    
    # Verify results
    logger.debug("Verifying results: %s", result)
    self.assertIsNotNone(result)
    logger.info("Complex workflow test completed successfully")

# Good - Capturing logs for verification
def test_logging_behavior(self):
    with self.assertLogs('module_name', level='INFO') as log:
        function_that_logs()
    
    self.assertIn('Expected log message', log.output[0])
```

## Test Quality Standards

### 1. Test Coverage
**Aim for high test coverage, but focus on meaningful tests.**

- **Target**: 80%+ line coverage
- **Focus**: Test business logic, not trivial getters/setters
- **Exclude**: Generated code, simple data classes, main functions

### 2. Test Independence
**Each test should be independent and not rely on other tests.**

```python
# Good - Independent tests
def test_feature_a(self):
    # Test feature A in isolation
    pass

def test_feature_b(self):
    # Test feature B in isolation
    pass

# Bad - Dependent tests
def test_feature_a(self):
    # Test feature A
    self.shared_state = "modified"

def test_feature_b(self):
    # Depends on shared_state from previous test
    self.assertEqual("modified", self.shared_state)  # expected, actual
```

### 3. Test Performance
**Keep tests fast and efficient.**

- **Unit tests**: < 1ms each
- **Integration tests**: < 100ms each
- **End-to-end tests**: < 1s each
- **Total test suite**: < 30s

### 4. Test Maintainability
**Write tests that are easy to understand and maintain.**

```python
# Good - Clear and maintainable
def test_calculate_score_with_half_passing_tests(self):
    # Given: 5 tests total, 3 passing
    total_tests = 5
    passing_tests = 3
    
    # When: calculating score
    score = calculate_score(passing_tests, total_tests)
    
    # Then: score should be 0.6
    self.assertEqual(0.6, score)  # expected, actual

# Bad - Unclear and hard to maintain
def test_score(self):
    self.assertEqual(0.6, calculate_score(3, 5))  # expected, actual
```

## Test Utilities and Helpers

### 1. Test Base Classes
**Use consistent base classes for common test patterns.**

```python
# Good - Consistent base class usage
class TestQualityAssuranceRunner(AsyncAutoCWDTestCase):
    async def asyncSetUp(self):
        self.runner = QualityAssuranceRunner()
        self.executor = MockExecutor()
```

### 2. Test Fixtures
**Use fixtures for common test setup.**

```python
# Good - Reusable fixtures
@pytest.fixture
def mock_executor():
    executor = Mock(spec=LayeredCommand)
    executor.execute.return_value = (0, "output", "error")
    return executor

def test_with_fixture(self, mock_executor):
    runner = QualityAssuranceRunner(executor=mock_executor)
    result = runner.run("target")
    self.assertEqual(1.0, result)  # expected, actual
```

### 3. Test Data Builders
**Use builders for complex test data.**

```python
# Good - Builder pattern for test data
class TestDataBuilder:
    def __init__(self):
        self.data = {}
    
    def with_version(self, version: str):
        self.data['version'] = version
        return self
    
    def with_dependencies(self, deps: List[str]):
        self.data['dependencies'] = deps
        return self
    
    def build(self):
        return TestData(**self.data)

# Usage
def test_with_builder(self):
    test_data = TestDataBuilder().with_version("1.0.0").with_dependencies(["pytest"]).build()
    result = process_data(test_data)
    self.assertIsNotNone(result)
```

### 4. Test File Management
**Use built-in Python utilities for test file management.**

```python
import tempfile
import os
from pathlib import Path

# Good - Using built-in tempfile
def test_file_operations(self):
    with tempfile.TemporaryDirectory() as temp_dir:
        test_file = Path(temp_dir) / "test.txt"
        test_file.write_text("test content")
        
        result = process_file(str(test_file))
        self.assertEqual("processed", result)  # expected, actual

# Good - Using built-in os for cleanup
def test_directory_operations(self):
    temp_dir = tempfile.mkdtemp()
    try:
        test_file = os.path.join(temp_dir, "test.txt")
        with open(test_file, 'w') as f:
            f.write("test content")
        
        result = process_directory(temp_dir)
        self.assertEqual("processed", result)  # expected, actual
    finally:
        import shutil
        shutil.rmtree(temp_dir)
```

## Anti-Patterns to Avoid

### 1. Test Smells
**Avoid these common test anti-patterns:**

- **Brittle Tests**: Tests that break when implementation changes
- **Slow Tests**: Tests that take too long to run
- **Flaky Tests**: Tests that sometimes pass, sometimes fail
- **Test Duplication**: Repeated test logic across multiple tests
- **Hidden Dependencies**: Tests that depend on external state

### 2. Code Smells
**Avoid these code patterns that make testing difficult:**

- **God Classes**: Classes with too many responsibilities
- **Tight Coupling**: Hard to replace dependencies
- **Global State**: Shared mutable state
- **Hidden Side Effects**: Unexpected behavior changes
- **Complex Constructors**: Hard to set up test objects

### 3. Logging Anti-Patterns
**Avoid these logging mistakes:**

```python
# Bad - Logging sensitive information
logger.info(f"User login: {username}, password: {password}")

# Bad - Excessive logging in tight loops
for item in large_list:
    logger.debug(f"Processing item: {item}")  # Too much noise

# Bad - Logging without context
logger.info("Error occurred")  # What error? Where?

# Good - Proper logging
logger.info("User login attempt: %s", username)  # No sensitive data
logger.debug("Processing batch of %d items", len(large_list))  # Periodic logging
logger.error("Database connection failed: %s", error)  # Clear context
```

## Testing Tools and Frameworks

### 1. Primary Testing Framework
**Use unittest for consistency with Python standard library.**

```python
import unittest
from unittest.mock import Mock, patch

class TestExample(unittest.TestCase):
    def test_example(self):
        # Test implementation
        pass
```

### 2. Mocking Framework
**Use unittest.mock for mocking and patching.**

```python
from unittest.mock import Mock, patch, MagicMock

# Mock objects
mock_executor = Mock(spec=LayeredCommand)

# Patch external dependencies
@patch('module.external_function')
def test_with_patch(self, mock_external):
    pass
```

### 3. Test Utilities
**Use built-in Python utilities for test infrastructure.**

```python
import tempfile
import os
import shutil
from pathlib import Path
from io import StringIO
from contextlib import redirect_stdout, redirect_stderr

# File operations
with tempfile.TemporaryDirectory() as temp_dir:
    # Test file operations
    pass

# Output capture
with redirect_stdout(StringIO()) as output:
    function_that_prints()
    self.assertIn("expected output", output.getvalue())
```

### 4. Alternative Testing Frameworks
**Consider pytest for more advanced testing needs.**

```python
# pytest example
def test_example():
    assert 1 + 1 == 2

# pytest with fixtures
@pytest.fixture
def sample_data():
    return {"key": "value"}

def test_with_fixture(sample_data):
    assert sample_data["key"] == "value"
```

## Continuous Integration

### 1. Test Execution
**Run tests automatically on code changes.**

- **Pre-commit**: Run fast tests before commits
- **CI Pipeline**: Run full test suite on pull requests
- **Nightly**: Run comprehensive test suite including performance tests

### 2. Test Reporting
**Generate and review test reports.**

- **Coverage Reports**: Track test coverage trends
- **Test Results**: Monitor test pass/fail rates
- **Performance Metrics**: Track test execution times

### 3. Test Maintenance
**Regularly maintain and update tests.**

- **Remove Dead Tests**: Delete tests for removed functionality
- **Update Tests**: Modify tests when requirements change
- **Refactor Tests**: Improve test quality and maintainability

## Logging Best Practices for Testing

### 1. When to Use Logging in Tests
**Use logging strategically in tests:**

- **Debug test failures**: Logs help understand why a test failed
- **Verify behavior**: Logs confirm the code is working as expected
- **Document flow**: Logs show the execution path through complex code
- **Performance monitoring**: Log timing information for performance tests

### 2. Logging Configuration for Tests
**Configure logging appropriately for test environments.**

```python
# Good - Test-specific logging configuration
import logging

def setup_test_logging():
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

# Good - Capturing logs in tests
def test_logging_behavior(self):
    with self.assertLogs('module_name', level='INFO') as log:
        function_that_logs()
    
    self.assertIn('Expected log message', log.output[0])
```

### 3. Logging Performance in Tests
**Use lazy evaluation for expensive logging operations.**

```python
# Good - Lazy evaluation
logger.debug("Processing %d items", len(expensive_list))

# Bad - Immediate evaluation
logger.debug(f"Processing {len(expensive_list)} items")
```

## Summary

These rules ensure that:

1. **Code is testable**: Easy to test in isolation
2. **Tests are reliable**: Consistent and predictable results
3. **Tests are maintainable**: Easy to understand and modify
4. **Tests are fast**: Quick feedback during development
5. **Tests are comprehensive**: Cover all important scenarios
6. **Logging is strategic**: Provides value without being a burden

**Remember**: Good tests are an investment in code quality and maintainability. They catch bugs early, enable refactoring, and serve as living documentation of how the code should behave. Logs are valuable tools for debugging and understanding code flow - use them strategically, not excessively.

**Critical Note**: Always use `assertEqual(expected, actual)` - the first argument is the expected value, the second is the actual value. This ensures clear error messages when tests fail.
# Test Rules and Guidelines

## Overview

This document defines comprehensive rules for creating testable code and managing tests in Python projects. These rules ensure that code is maintainable, reliable, and follows best practices for testing.

## Code Structure for Testability

### 1. Dependency Injection
**ALWAYS inject dependencies rather than creating them internally.**

```python
# Good - Testable
class QualityAssuranceRunner:
    def __init__(self, executor: LayeredCommand, logger: Logger):
        self.executor = executor
        self.logger = logger

# Bad - Not testable
class QualityAssuranceRunner:
    def __init__(self):
        self.executor = LayeredCommand()  # Hard to mock
        self.logger = logging.getLogger(__name__)  # Hard to control
```

### 2. Interface Segregation
**Create small, focused interfaces that are easy to mock.**

```python
# Good - Small, focused interface
class Executor(Protocol):
    def execute(self, command: str) -> Tuple[int, str, str]:
        ...

# Bad - Large, complex interface
class Executor(Protocol):
    def execute(self, command: str) -> Tuple[int, str, str]:
        ...
    def install_dependencies(self, deps: List[str]) -> None:
        ...
    def setup_environment(self, env: Dict[str, str]) -> None:
        ...
```

### 3. Pure Functions
**Prefer pure functions that are deterministic and have no side effects.**

```python
# Good - Pure function
def calculate_score(passed: int, total: int) -> float:
    if total == 0:
        return 0.0
    return passed / total

# Bad - Function with side effects
def calculate_score(passed: int, total: int) -> float:
    if total == 0:
        print("No tests found")  # Side effect
        return 0.0
    return passed / total
```

### 4. Error Handling
**Make error conditions explicit and testable.**

```python
# Good - Explicit error handling
def parse_version(version_str: str) -> Version:
    try:
        parts = version_str.split(".")
        if len(parts) != 3:
            raise ValueError(f"Version must have 3 parts, got {len(parts)}")
        return Version(*map(int, parts))
    except ValueError as e:
        raise ValueError(f"Invalid version format: {version_str}") from e

# Bad - Implicit error handling
def parse_version(version_str: str) -> Version:
    parts = version_str.split(".")
    return Version(*map(int, parts))  # Can fail silently
```

### 5. Logging in Testable Code
**Use logging strategically - it's valuable for debugging and understanding code flow.**

```python
# Good - Strategic logging
def calculate_score(passed: int, total: int) -> float:
    logger.debug("Calculating score: %d passed out of %d total", passed, total)
    if total == 0:
        logger.warning("No tests found, returning default score")
        return 0.0
    
    score = passed / total
    logger.info("Test score calculated: %.2f", score)
    return score

# Good - Logging with lazy evaluation for performance
def process_large_dataset(items: List[str]) -> List[str]:
    logger.info("Processing %d items", len(items))  # Lazy evaluation
    for i, item in enumerate(items):
        if i % 1000 == 0:  # Log progress periodically
            logger.debug("Processed %d/%d items", i, len(items))
        # Process item
    logger.info("Processing completed")
```

**When to use logging in tests:**
- **Debug test failures**: Logs help understand why a test failed
- **Verify behavior**: Logs confirm the code is working as expected
- **Document flow**: Logs show the execution path through complex code
- **Performance monitoring**: Log timing information for performance tests

## Test Organization

### 1. Test Structure
**Follow the AAA pattern: Arrange, Act, Assert.**

```python
def test_calculate_score_perfect_score(self):
    # Arrange
    passed = 10
    total = 10
    
    # Act
    result = calculate_score(passed, total)
    
    # Assert
    self.assertEqual(1.0, result)  # expected, actual
```

### 2. Test Naming
**Use descriptive test names that explain the scenario and expected outcome.**

```python
# Good - Descriptive names
def test_calculate_score_with_zero_total_returns_zero(self):
def test_parse_version_with_invalid_format_raises_value_error(self):
def test_quality_assurance_runner_with_failing_tests_raises_exit_early_error(self):

# Bad - Vague names
def test_calculate_score(self):
def test_parse_version(self):
def test_runner(self):
```

### 3. Test Categories
**Organize tests by category and complexity.**

- **Unit Tests**: Test individual functions/classes in isolation
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete workflows
- **Property Tests**: Test invariants and properties
- **Performance Tests**: Test timing and resource usage

### 4. Test Data Management
**Use factories and builders for test data creation.**

```python
# Good - Test data factory
class VersionFactory:
    @staticmethod
    def create(major: int = 1, minor: int = 0, patch: int = 0) -> Version:
        return Version(major, minor, patch)
    
    @staticmethod
    def create_invalid() -> str:
        return "invalid.version"

# Usage in tests
def test_version_creation(self):
    version = VersionFactory.create(major=2, minor=1, patch=3)
    self.assertEqual(2, version.major)  # expected, actual
```

## Testing Strategies

### 1. Mocking Guidelines
**Mock external dependencies, not the code under test.**

```python
# Good - Mock external dependencies
@patch('quickpub.proxy.os_system')
def test_runner_execution(self, mock_os_system):
    mock_os_system.return_value = (0, "output", "error")
    
    runner = QualityAssuranceRunner()
    result = runner.run("target")
    
    self.assertEqual(1.0, result)  # expected, actual

# Bad - Mock the code under test
@patch('quickpub.quality_assurance_runner.QualityAssuranceRunner._calculate_score')
def test_runner_execution(self, mock_calculate):
    mock_calculate.return_value = 1.0
    # This doesn't test the actual logic
```

### 2. Async Testing
**Use proper async test patterns for async code.**

```python
# Good - Async test pattern
class TestAsyncRunner(AsyncAutoCWDTestCase):
    async def asyncSetUp(self):
        self.runner = AsyncQualityAssuranceRunner()
    
    async def test_async_execution(self):
        result = await self.runner.run_async("target")
        self.assertEqual(1.0, result)  # expected, actual
```

### 3. Error Testing
**Test both success and failure scenarios.**

```python
def test_success_scenario(self):
    # Test normal operation
    result = function_under_test(valid_input)
    self.assertEqual(expected_output, result)  # expected, actual

def test_error_scenario(self):
    # Test error conditions
    with self.assertRaises(ValueError):
        function_under_test(invalid_input)
```

### 4. Boundary Testing
**Test edge cases and boundary conditions.**

```python
def test_boundary_conditions(self):
    # Test minimum values
    result = calculate_score(0, 0)
    self.assertEqual(0.0, result)  # expected, actual
    
    # Test maximum values
    result = calculate_score(100, 100)
    self.assertEqual(1.0, result)  # expected, actual
    
    # Test edge cases
    result = calculate_score(1, 1)
    self.assertEqual(1.0, result)  # expected, actual
```

### 5. Logging in Tests
**Use logging strategically in tests for debugging and verification.**

```python
# Good - Logging for test debugging
def test_complex_workflow(self):
    logger.info("Starting complex workflow test")
    
    # Test setup
    logger.debug("Setting up test data")
    test_data = create_test_data()
    
    # Execute workflow
    logger.debug("Executing workflow with %d items", len(test_data))
    result = execute_workflow(test_data)
    
    # Verify results
    logger.debug("Verifying results: %s", result)
    self.assertIsNotNone(result)
    logger.info("Complex workflow test completed successfully")

# Good - Capturing logs for verification
def test_logging_behavior(self):
    with self.assertLogs('module_name', level='INFO') as log:
        function_that_logs()
    
    self.assertIn('Expected log message', log.output[0])
```

## Test Quality Standards

### 1. Test Coverage
**Aim for high test coverage, but focus on meaningful tests.**

- **Target**: 80%+ line coverage
- **Focus**: Test business logic, not trivial getters/setters
- **Exclude**: Generated code, simple data classes, main functions

### 2. Test Independence
**Each test should be independent and not rely on other tests.**

```python
# Good - Independent tests
def test_feature_a(self):
    # Test feature A in isolation
    pass

def test_feature_b(self):
    # Test feature B in isolation
    pass

# Bad - Dependent tests
def test_feature_a(self):
    # Test feature A
    self.shared_state = "modified"

def test_feature_b(self):
    # Depends on shared_state from previous test
    self.assertEqual("modified", self.shared_state)  # expected, actual
```

### 3. Test Performance
**Keep tests fast and efficient.**

- **Unit tests**: < 1ms each
- **Integration tests**: < 100ms each
- **End-to-end tests**: < 1s each
- **Total test suite**: < 30s

### 4. Test Maintainability
**Write tests that are easy to understand and maintain.**

```python
# Good - Clear and maintainable
def test_calculate_score_with_half_passing_tests(self):
    # Given: 5 tests total, 3 passing
    total_tests = 5
    passing_tests = 3
    
    # When: calculating score
    score = calculate_score(passing_tests, total_tests)
    
    # Then: score should be 0.6
    self.assertEqual(0.6, score)  # expected, actual

# Bad - Unclear and hard to maintain
def test_score(self):
    self.assertEqual(0.6, calculate_score(3, 5))  # expected, actual
```

## Test Utilities and Helpers

### 1. Test Base Classes
**Use consistent base classes for common test patterns.**

```python
# Good - Consistent base class usage
class TestQualityAssuranceRunner(AsyncAutoCWDTestCase):
    async def asyncSetUp(self):
        self.runner = QualityAssuranceRunner()
        self.executor = MockExecutor()
```

### 2. Test Fixtures
**Use fixtures for common test setup.**

```python
# Good - Reusable fixtures
@pytest.fixture
def mock_executor():
    executor = Mock(spec=LayeredCommand)
    executor.execute.return_value = (0, "output", "error")
    return executor

def test_with_fixture(self, mock_executor):
    runner = QualityAssuranceRunner(executor=mock_executor)
    result = runner.run("target")
    self.assertEqual(1.0, result)  # expected, actual
```

### 3. Test Data Builders
**Use builders for complex test data.**

```python
# Good - Builder pattern for test data
class TestDataBuilder:
    def __init__(self):
        self.data = {}
    
    def with_version(self, version: str):
        self.data['version'] = version
        return self
    
    def with_dependencies(self, deps: List[str]):
        self.data['dependencies'] = deps
        return self
    
    def build(self):
        return TestData(**self.data)

# Usage
def test_with_builder(self):
    test_data = TestDataBuilder().with_version("1.0.0").with_dependencies(["pytest"]).build()
    result = process_data(test_data)
    self.assertIsNotNone(result)
```

### 4. Test File Management
**Use built-in Python utilities for test file management.**

```python
import tempfile
import os
from pathlib import Path

# Good - Using built-in tempfile
def test_file_operations(self):
    with tempfile.TemporaryDirectory() as temp_dir:
        test_file = Path(temp_dir) / "test.txt"
        test_file.write_text("test content")
        
        result = process_file(str(test_file))
        self.assertEqual("processed", result)  # expected, actual

# Good - Using built-in os for cleanup
def test_directory_operations(self):
    temp_dir = tempfile.mkdtemp()
    try:
        test_file = os.path.join(temp_dir, "test.txt")
        with open(test_file, 'w') as f:
            f.write("test content")
        
        result = process_directory(temp_dir)
        self.assertEqual("processed", result)  # expected, actual
    finally:
        import shutil
        shutil.rmtree(temp_dir)
```

## Anti-Patterns to Avoid

### 1. Test Smells
**Avoid these common test anti-patterns:**

- **Brittle Tests**: Tests that break when implementation changes
- **Slow Tests**: Tests that take too long to run
- **Flaky Tests**: Tests that sometimes pass, sometimes fail
- **Test Duplication**: Repeated test logic across multiple tests
- **Hidden Dependencies**: Tests that depend on external state

### 2. Code Smells
**Avoid these code patterns that make testing difficult:**

- **God Classes**: Classes with too many responsibilities
- **Tight Coupling**: Hard to replace dependencies
- **Global State**: Shared mutable state
- **Hidden Side Effects**: Unexpected behavior changes
- **Complex Constructors**: Hard to set up test objects

### 3. Logging Anti-Patterns
**Avoid these logging mistakes:**

```python
# Bad - Logging sensitive information
logger.info(f"User login: {username}, password: {password}")

# Bad - Excessive logging in tight loops
for item in large_list:
    logger.debug(f"Processing item: {item}")  # Too much noise

# Bad - Logging without context
logger.info("Error occurred")  # What error? Where?

# Good - Proper logging
logger.info("User login attempt: %s", username)  # No sensitive data
logger.debug("Processing batch of %d items", len(large_list))  # Periodic logging
logger.error("Database connection failed: %s", error)  # Clear context
```

## Testing Tools and Frameworks

### 1. Primary Testing Framework
**Use unittest for consistency with Python standard library.**

```python
import unittest
from unittest.mock import Mock, patch

class TestExample(unittest.TestCase):
    def test_example(self):
        # Test implementation
        pass
```

### 2. Mocking Framework
**Use unittest.mock for mocking and patching.**

```python
from unittest.mock import Mock, patch, MagicMock

# Mock objects
mock_executor = Mock(spec=LayeredCommand)

# Patch external dependencies
@patch('module.external_function')
def test_with_patch(self, mock_external):
    pass
```

### 3. Test Utilities
**Use built-in Python utilities for test infrastructure.**

```python
import tempfile
import os
import shutil
from pathlib import Path
from io import StringIO
from contextlib import redirect_stdout, redirect_stderr

# File operations
with tempfile.TemporaryDirectory() as temp_dir:
    # Test file operations
    pass

# Output capture
with redirect_stdout(StringIO()) as output:
    function_that_prints()
    self.assertIn("expected output", output.getvalue())
```

### 4. Alternative Testing Frameworks
**Consider pytest for more advanced testing needs.**

```python
# pytest example
def test_example():
    assert 1 + 1 == 2

# pytest with fixtures
@pytest.fixture
def sample_data():
    return {"key": "value"}

def test_with_fixture(sample_data):
    assert sample_data["key"] == "value"
```

## Continuous Integration

### 1. Test Execution
**Run tests automatically on code changes.**

- **Pre-commit**: Run fast tests before commits
- **CI Pipeline**: Run full test suite on pull requests
- **Nightly**: Run comprehensive test suite including performance tests

### 2. Test Reporting
**Generate and review test reports.**

- **Coverage Reports**: Track test coverage trends
- **Test Results**: Monitor test pass/fail rates
- **Performance Metrics**: Track test execution times

### 3. Test Maintenance
**Regularly maintain and update tests.**

- **Remove Dead Tests**: Delete tests for removed functionality
- **Update Tests**: Modify tests when requirements change
- **Refactor Tests**: Improve test quality and maintainability

## Logging Best Practices for Testing

### 1. When to Use Logging in Tests
**Use logging strategically in tests:**

- **Debug test failures**: Logs help understand why a test failed
- **Verify behavior**: Logs confirm the code is working as expected
- **Document flow**: Logs show the execution path through complex code
- **Performance monitoring**: Log timing information for performance tests

### 2. Logging Configuration for Tests
**Configure logging appropriately for test environments.**

```python
# Good - Test-specific logging configuration
import logging

def setup_test_logging():
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

# Good - Capturing logs in tests
def test_logging_behavior(self):
    with self.assertLogs('module_name', level='INFO') as log:
        function_that_logs()
    
    self.assertIn('Expected log message', log.output[0])
```

### 3. Logging Performance in Tests
**Use lazy evaluation for expensive logging operations.**

```python
# Good - Lazy evaluation
logger.debug("Processing %d items", len(expensive_list))

# Bad - Immediate evaluation
logger.debug(f"Processing {len(expensive_list)} items")
```

## Summary

These rules ensure that:

1. **Code is testable**: Easy to test in isolation
2. **Tests are reliable**: Consistent and predictable results
3. **Tests are maintainable**: Easy to understand and modify
4. **Tests are fast**: Quick feedback during development
5. **Tests are comprehensive**: Cover all important scenarios
6. **Logging is strategic**: Provides value without being a burden

**Remember**: Good tests are an investment in code quality and maintainability. They catch bugs early, enable refactoring, and serve as living documentation of how the code should behave. Logs are valuable tools for debugging and understanding code flow - use them strategically, not excessively.

**Critical Note**: Always use `assertEqual(expected, actual)` - the first argument is the expected value, the second is the actual value. This ensures clear error messages when tests fail.
